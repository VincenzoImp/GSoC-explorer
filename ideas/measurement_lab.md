# Measurement Lab ‚Äî Project Ideas

**Source:** https://github.com/m-lab/gsoc
**Scraped:** 2026-02-22T23:28:47.601538

---

Welcome to M-Lab's Google Summer of Code page! We're excited to have you here and look forward to working with talented contributors from around the world.

**Important notice (2026-02-20)**: We have experienced a significant and
growing volume of unsolicited pull requests and issues across our repositories
coinciding with the GSoC 2026 application period. Many of these contributions
appear to be automated or AI-generated, and the volume has become
unsustainable for our small team of maintainers.

**Effective immediately, we are pausing review of all unsolicited pull
requests and issues** ‚Äî that is, any PR or issue that was not coordinated
with a maintainer in advance. We need time to catch up and to ensure that
genuine contributors are not drowned out.

If you are a prospective GSoC applicant, please read the updated
[engagement requirements](https://github.com#2-engage-with-us) below carefully. Going forward,
we require prior contact and coordination before any code contribution.
Opening pull requests or issues without prior discussion is not a productive
way to engage with us and will not strengthen your application.

Measurement Lab (M-Lab) [https://www.measurementlab.net/](https://www.measurementlab.net/) is an open source project with contributors from civil society organizations, educational institutions, and private sector companies dedicated to:

- Providing an open, verifiable measurement platform for global network performance
- Hosting the largest open Internet performance dataset on the planet
- Creating visualizations and tools to help people make sense of Internet performance

**Mentorship**: Work closely with experienced developers and active community members**Real Impact**: M-Lab's tools are used by a large community of researchers, policymakers, and open Internet advocates. Your contributions can become a core part of M-Lab's tools.**Skill Development**: Gain hands-on experience with Python, JavaScript and other Web technologies, large datasets (SQL, BigQuery)**Open Source**: All work is open source and publicly available

Before applying, we strongly encourage you to:

- Explore our
[main repository](https://github.com/orgs/m-lab/repositories)and the repositories of the proposed projects (see below) - Visit our
[website](https://www.measurementlab.net/)and read about our[activities](https://www.measurementlab.net/about/),[news](https://www.measurementlab.net/blog/), and documentation of our[tools](https://www.measurementlab.net/tests/)and[data](https://www.measurementlab.net/data/) - Join our communication channels:
- Mailing list:
[link](https://groups.google.com/a/measurementlab.net/g/discuss)

- Mailing list:

**Quality of thinking matters far more than quantity of commits.** We select
contributors, not proposals and not pull requests. We want to work with people
who understand our project, can hold a technical conversation, and are
genuinely interested in our mission.

Before opening any pull request, issue, or submitting a GSoC proposal for
M-Lab, you **must** complete the following steps:

-
**Email us first.**Write to[support@measurementlab.net](mailto:support@measurementlab.net)with the subject linefollowed by a concise project title. In your email, introduce yourself, describe the project idea you are interested in, and explain your relevant background. This email should demonstrate that you have studied our project ideas and documentation.`[GSoC2026]`

-
**Be ready for a conversation.**Mentors may follow up with questions or a short call. You should be able to discuss the project in your own words, explain your technical approach, and answer questions about your background. We are evaluating whether we can work together productively for several months. -
**Coordinate before contributing code.**Do not open pull requests or issues without prior agreement from a maintainer. When a maintainer confirms that a specific contribution would be welcome, they will tell you how to proceed. -
**One-two focused contributions.**We expect at most 1-2 pull requests or issues per contributor during the application period, and only after coordination with a maintainer. Bulk submissions, drive-by PRs, and unsolicited ‚Äúfix‚Äù contributions are counterproductive and will not be considered favorably.

-
Opening pull requests or issues without prior discussion with a maintainer

-
Submitting AI-generated or low-effort contributions

-
Opening multiple PRs or issues to ‚Äúshow activity‚Äù

-
Contacting us just to ‚Äúintroduce yourself‚Äù without having done prior research

-
Posting proposal ideas directly to the

[mailing list](https://groups.google.com/a/measurementlab.net/g/discuss)

Flooding our repositories with unsolicited contributions does not demonstrate engagement ‚Äî it creates work for our maintainers and takes time away from mentoring genuine applicants.

We are not opposed to the thoughtful use of AI tools. However, the volume of unreviewed, AI-generated contributions we have received during this application period has made it necessary to set explicit expectations.

**Disclosure requirement.** All code contributions and proposals must include
an AI Assistance Disclosure if AI tools were used in any part of the work.
State what was AI-generated, how AI was used, and confirm that you have
reviewed and understand the output. Omitting disclosure when AI was used
is grounds for immediate rejection.

**Proposals.** Your proposal must reflect your own thinking. You may use AI
tools for grammar and clarity improvements, but the substance of your
proposal ‚Äî the technical approach, the timeline, the design decisions ‚Äî must
be your own work. Proposals that read as raw AI output will be rejected
without further review.

**Communication with reviewers.** Do not, *under any circumstances*, use AI
to respond to review comments or communicate with maintainers. We have
received numerous AI-generated replies to code reviews and emails, and they
are obvious. Genuine engagement with feedback is a core part of how we
evaluate contributors. AI-generated replies to reviews are disrespectful to
the review process and will be treated as grounds for rejection.

**Quality standards.** Using AI does not lower our standards. We will reject
contributions that exhibit signs of unreviewed AI output, including but not
limited to:

- Large auto-generated summaries or commit messages
- Verbose, boilerplate documentation that adds no real value
- Over-engineered code with unnecessary abstractions
- Trivial or redundant tests
- Unreviewed boilerplate that does not fit the project's conventions

**Understanding requirement.** You must fully understand every line of code
and every design decision in your contributions. During the program, we will
have live discussions about your work. If you cannot explain what you
submitted, it will be evident.

**Enforcement.** Failure to disclose AI usage, submit reviewed work, or
demonstrate understanding of your contributions may result in immediate
rejection of your application or removal from the program.

We may not be able to respond to all inquiries ‚Äî particularly those that do not demonstrate prior research or substantive preparation.

A strong proposal should include:

**Personal Information**: Name, contact details, time zone, GitHub username**Background**: Your relevant experience, skills, and courses**Project Selection**: Which project idea(s) interest you and why**Project Plan**:- Clear objectives and deliverables
- Weekly timeline with milestones
- Technical approach and implementation details
- Testing strategy
- Documentation plan

**Availability**: Expected hours per week, any planned absences**Why You**: What makes you a good fit for this project**Post-GSoC**: Your plans for continued involvement

- Application period: Feb 19 to Mar 31 (see official
[timeline](https://developers.google.com/open-source/gsoc/timeline)) - Submit through the
[official GSoC website](https://summerofcode.withgoogle.com/) - Deadline: 31 March 2026
- You may submit up to 3 proposals total (across all organizations)

**Be respectful**: Follow our Code of Conduct**Be patient**: Mentors are volunteers and may take time to respond**Be proactive**: Don't wait to be told what to do**Ask smart questions**: Show what you've already tried**Use public channels**: So others can learn and help too

Below are project ideas for GSoC 2026. These are starting points - we encourage you to propose your own ideas or variations on these themes!

**Difficulty Levels:**

- üü¢ Easy (90 hours) - Good for first-time GSoC participants
- üü° Medium (175 hours) - Requires some project familiarity
- üî¥ Hard (350 hours) - Complex projects requiring deep expertise

**Difficulty**: üü° Medium (175 hours)

**Mentor(s)**: Pavlos Sermpezis, Roberto D'Auria, Simone Basso
**Skills Required**: Python 3.10+, Docker, Poetry, Git, CI/CD (GitHub Actions), Network measurement

**Difficulty**: üî¥ Hard (350 hours)

**Mentor(s)**: Pavlos Sermpezis, Roberto D'Auria, Simone Basso

**Skills Required**: Python, Data Visualization (Plotly), Streamlit, Pandas

**Skills Preferred**: Geographic data visualization, BigQuery/SQL, UX/UI design, Statistics

The Internet Quality Barometer (IQB) is M-Lab's comprehensive framework for measuring Internet performance across various use cases (web browsing, video streaming, gaming, etc.). Unlike simple "speed tests," IQB calculates a composite score reflecting the quality of Internet experience holistically. Currently, IQB has a prototype Streamlit application for applying the framework, but it lacks comprehensive dashboards for analyzing and comparing ISP performance across geographic regions.

This project involves extending the IQB platform by creating interactive, user-friendly dashboards that provide actionable insights into ISP performance. You'll design and implement visualizations that allow users to compare ISPs within countries and cities, identify performance trends, and make informed decisions about Internet service quality. The dashboards should serve multiple user personas: consumers choosing ISPs, researchers studying Internet quality, policymakers monitoring infrastructure, and ISPs benchmarking their performance.

You'll work with M-Lab's extensive measurement dataset, implement statistical aggregations, create interactive geographic visualizations, and design intuitive comparison tools. This is an opportunity to impact how millions understand and choose their Internet services based on real measurement data.

**ISP Performance Dashboard**: Create a comprehensive dashboard showing ISP performance metrics (IQB score, latency, throughput, packet loss) with time-series trends, percentile distributions, and sample size indicators**Geographic Comparison Tool**: Build interactive map-based visualizations showing ISP performance across countries, regions, and cities with drill-down capabilities and geographic heatmaps**ISP Head-to-Head Comparison**: Implement a side-by-side comparison feature allowing users to compare 2-5 ISPs across multiple metrics with statistical significance testing**Use Case Performance Analysis**: Create specialized views showing how ISPs perform for specific use cases (video streaming, gaming, video conferencing, web browsing) based on IQB's multi-dimensional framework**Data Export and Reporting**: Add functionality to export visualizations, generate PDF reports, and download filtered datasets for further analysis**User Experience Design**: Design an intuitive interface with responsive layouts, clear data presentation, and guided workflows for different user types**Performance Optimization**: Implement efficient data caching, lazy loading, and aggregation strategies to handle large M-Lab datasets without lag**Documentation**: Create user guides, API documentation, and developer documentation for extending the dashboard system

**Difficulty**: üü° Medium (175 hours)

**Mentor(s)**: Pavlos Sermpezis, Roberto D'Auria, Simone Basso

**Skills Required**: JavaScript, Angular.js, HTML/CSS, REST APIs

M-Lab's speed test at speed.measurementlab.net is one of the most widely used open-source Internet measurement tools globally. Currently, after running a test, users receive basic metrics (download/upload speeds, latency) but limited context about what these numbers mean or how they compare to others. This project enhances the results page to provide actionable insights and meaningful comparisons.

You'll extend the speed test interface to calculate and display IQB (Internet Quality Barometer) scores based on test results, explain what the results mean for different use cases (streaming, gaming, video calls), and show how the user's performance compares to their ISP's average and regional benchmarks. This makes the speed test results more meaningful and helps users understand their Internet quality in practical terms.

**Difficulty**: üî¥ Hard (350 hours)

**Mentor(s)**: Pavlos Sermpezis, Roberto D'Auria, Simone Basso

**Skills Required**: JavaScript, HTML/CSS, REST APIs, Authentication (OAuth/Firebase Auth), Database design, Data visualization, Security best practices

This extended version of Project 3A includes all the features from the medium project (IQB scores, insights, comparisons) plus a comprehensive user authentication system and personalized dashboard for tracking measurement history over time.

Users will be able to create accounts, run tests while logged in, and access a personal dashboard showing their historical test results, trends over time, comparisons across different locations/networks, and personalized recommendations. This transforms the speed test from a one-time measurement tool into a long-term monitoring solution that helps users track their Internet quality, identify patterns, and make informed decisions about their service.

You'll implement secure user authentication, design a database schema for storing user measurements, build a personal dashboard with rich visualizations, ensure privacy compliance (GDPR, data retention policies), and maintain the existing functionality for anonymous users who don't want to create accounts.

We evaluate applicants holistically. We choose the contributor, not the proposal, not the pull request. Our criteria, roughly in order of importance:

-
**Genuine engagement**: Did you communicate with us? Can you discuss the project knowledgeably? Do you understand what M-Lab does and why? -
**Communication**: Clarity, responsiveness, professionalism, ability to hold a technical conversation, kindness, empathy. -
**Relevant experience**: Skills and background relevant to the project. -
**Proposal quality**: Clear goals, realistic timeline, technical soundness.

We may conduct short interviews with shortlisted candidates to assess fit.

No, GSoC allows you to work on only one project. However, you can submit proposals for up to 3 different projects across all organizations.

Absolutely! We have projects for all skill levels. Start with our "good first issues" and don't hesitate to ask questions.

This depends on the project. See individual project descriptions for required and preferred skills.

You must contact us at [support@measurementlab.net](mailto:support@measurementlab.net)
before contributing code. See the [engagement requirements](https://github.com#2-engage-with-us) above. Coordinated
contributions strengthen your application; unsolicited pull requests do not.

Communication is key. If you face challenges, reach out to your mentors immediately. We're here to help you succeed.

GSoC is open to new open-source contributors who are 18 years or older. You don't need to be enrolled in a university.

**Good luck with your application!** üöÄ
